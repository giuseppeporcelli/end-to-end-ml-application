{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Training</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Amazon SageMaker built-in XGBoost algorithm (https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to train a simple binary classification model, using the pre-processed data generated in the previous step by the AWS Glue job. Let's define some variables first.\n",
    "\n",
    "<span style=\"color: red\"> Please replace your initials in the bucket_name variable defined in next cell.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-west-1\n",
      "arn:aws:iam::825935527263:role/service-role/AmazonSageMaker-ExecutionRole-endtoendml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    "\n",
    "# replace [your-initials] according to the bucket name you have defined.\n",
    "bucket_name = 'endtoendml-workshop-[your-initials]'\n",
    "key_prefix = 'data/preprocessed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a look at our preprocessed data, which have already been split into training and validation sets by the AWS Glue job. In order to do that, we first download the preprocessed training file from Amazon S3 to the local notebook file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# The name of the file has been set by AWS Glue job in the previous notebook.\n",
    "train_file_name = 'part-00000'\n",
    "train_file_key = '{0}/train/{1}'.format(key_prefix, train_file_name)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket_name).download_file(train_file_key, train_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the file has been download, we can use Pandas to read the CSV and display the first 10 rows. You can immediately notice that categorical features have been one-hot encoded according to the feature engineering actions executed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...    19    20   21  \\\n",
       "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  12.0  39.0  0.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   1.0  64.0  0.0   \n",
       "2  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   4.0  63.0  0.0   \n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  13.0  85.0  0.0   \n",
       "4  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   6.0  77.0  0.0   \n",
       "5  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  15.0  49.0  0.0   \n",
       "6  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   8.0  81.0  0.0   \n",
       "7  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  13.0  40.0  0.0   \n",
       "8  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   9.0  69.0  0.0   \n",
       "9  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   4.0  22.0  0.0   \n",
       "\n",
       "    22   23   24   25   26   27   28  \n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "1  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "4  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "6  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "8  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "9  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.read_csv(train_file_name, header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start training, we need to specify the location of the docker container that will be used for training.\n",
    "Docker Registry paths for Amazon algorithms are specified here: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html\n",
    "\n",
    "By the way, we can use a utility function of the Amazon SageMaker Python SDK to get the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image.To use the newer image, please set 'repo_version'='0.90-1. For example:\n",
      "\tget_image_uri(region, 'xgboost', 0.90-1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version=\"latest\")\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training, by specifying the input and output settings and the required hyperparameters. You can find the list of the supported hyperparameters for the XGBoost algorithm here: https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html.\n",
    "\n",
    "You can also try running the following cell multiple times changing hyperparameters or other settings like the number of instances to be used for training, since XGBoost can be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-05 23:20:55 Starting - Starting the training job...\n",
      "2019-09-05 23:20:56 Starting - Launching requested ML instances...\n",
      "2019-09-05 23:21:52 Starting - Preparing the instances for training......\n",
      "2019-09-05 23:22:53 Downloading - Downloading input data\n",
      "2019-09-05 23:22:53 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-09-05:23:22:53:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-09-05:23:22:53:INFO] File size need to be processed in the node: 117.53mb. Available memory size in the node: 23674.17mb\u001b[0m\n",
      "\u001b[31m[2019-09-05:23:22:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[23:22:53] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[23:22:54] 799931x28 matrix with 22398068 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[2019-09-05:23:22:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[23:22:54] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[23:22:54] 200069x28 matrix with 5601932 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[23:22:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[0]#011train-rmse:0.354348#011validation-rmse:0.354348\u001b[0m\n",
      "\u001b[31m[23:22:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[1]#011train-rmse:0.256429#011validation-rmse:0.256429\u001b[0m\n",
      "\u001b[31m[23:22:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[2]#011train-rmse:0.187238#011validation-rmse:0.187238\u001b[0m\n",
      "\u001b[31m[23:22:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[3]#011train-rmse:0.137389#011validation-rmse:0.137389\u001b[0m\n",
      "\u001b[31m[23:22:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[4]#011train-rmse:0.101114#011validation-rmse:0.101114\u001b[0m\n",
      "\u001b[31m[23:22:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[5]#011train-rmse:0.074563#011validation-rmse:0.074563\u001b[0m\n",
      "\u001b[31m[23:22:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[6]#011train-rmse:0.055057#011validation-rmse:0.055057\u001b[0m\n",
      "\u001b[31m[23:22:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[7]#011train-rmse:0.040691#011validation-rmse:0.040691\u001b[0m\n",
      "\u001b[31m[23:22:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[8]#011train-rmse:0.030094#011validation-rmse:0.030094\u001b[0m\n",
      "\u001b[31m[23:22:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[9]#011train-rmse:0.022268#011validation-rmse:0.022268\u001b[0m\n",
      "\u001b[31m[23:22:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[10]#011train-rmse:0.016483#011validation-rmse:0.016483\u001b[0m\n",
      "\u001b[31m[23:22:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[11]#011train-rmse:0.012204#011validation-rmse:0.012204\u001b[0m\n",
      "\u001b[31m[23:22:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[12]#011train-rmse:0.009038#011validation-rmse:0.009038\u001b[0m\n",
      "\u001b[31m[23:22:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[13]#011train-rmse:0.006695#011validation-rmse:0.006695\u001b[0m\n",
      "\u001b[31m[23:22:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[14]#011train-rmse:0.00496#011validation-rmse:0.00496\u001b[0m\n",
      "\u001b[31m[23:22:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[15]#011train-rmse:0.003675#011validation-rmse:0.003675\u001b[0m\n",
      "\u001b[31m[23:22:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[16]#011train-rmse:0.002724#011validation-rmse:0.002724\u001b[0m\n",
      "\u001b[31m[23:22:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[17]#011train-rmse:0.002019#011validation-rmse:0.002019\u001b[0m\n",
      "\u001b[31m[23:23:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[18]#011train-rmse:0.001498#011validation-rmse:0.001498\u001b[0m\n",
      "\u001b[31m[23:23:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[19]#011train-rmse:0.001111#011validation-rmse:0.001111\u001b[0m\n",
      "\n",
      "2019-09-05 23:23:10 Uploading - Uploading generated training model\n",
      "2019-09-05 23:23:10 Completed - Training job completed\n",
      "Billable seconds: 40\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "output_location = 's3://{0}/output'.format(bucket_name)\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.2xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    base_job_name='predmain-train-xgb')\n",
    "\n",
    "est.set_hyperparameters(objective='reg:logistic',\n",
    "                        num_round=20)\n",
    "\n",
    "train_config = sagemaker.session.s3_input('s3://{0}/{1}/train/'.format(\n",
    "    bucket_name, key_prefix), content_type='text/csv')\n",
    "val_config = sagemaker.session.s3_input('s3://{0}/{1}/val/'.format(\n",
    "    bucket_name, key_prefix), content_type='text/csv')\n",
    "\n",
    "est.fit({'train': train_config, 'validation': val_config })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is completed, the serialized model will be saved in the S3 output_location defined above.\n",
    "You can now move to the next notebook in the **04_deploy_model** folder to see how to use that model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
